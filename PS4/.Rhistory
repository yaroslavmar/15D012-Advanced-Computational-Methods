probeExpanded))^p) )^(1/p)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- test[obs,]
probeExpanded <- matrix(probe, nrow = nrow(features), ncol = ncol(test), byrow = TRUE)
probeExpanded<-apply(probeExpanded,2,as.numeric)
# computing distances between the probe and exemplars in train dataset(features)
if (p %in% c(1,2)) {
distMatrix[,obs] <- ( rowSums((abs(features -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs,] <- apply(abs(features - probeExpanded), 1, max)
}
}
neighbors <- apply(as.matrix(distMatrix), 2, order)
neighbors <- neighbors[1:k,]
#rm(distMatrix)
n<-nrow(test)
neighbors.class<-sapply(1:n, function(i){
labels[neighbors[,i]]
})
# Frequency of each class
n.cat<-length(table(labels))
freq<-matrix(0, n, n.cat)
colnames(freq)<-names(table(labels))
for( i in 1:n){
categ<-table(match(as.factor(neighbors.class[,i]), colnames(freq)))
if(length(categ)>1) {
#freq[i,]<-categ
freq[i,as.numeric(names(categ))]<-categ
} else {
#freq[i,as.numeric(names(categ))]<-categ
freq[i,as.numeric(names(categ))]<-categ
}
}
# predicted label
PredLabels<- rep(NA,n)
for(i in 1:n){
PredLabels[i]<-colnames(freq)[which.max(freq[i,])]
}
#Probability of each class
prob<-freq/k
table(PredLabels)
#Version 4
kNN<-function(features, test,  labels, k, p){
# Compute distance
noObs <- nrow(test)
distMatrix <- matrix(NA, nrow(features), noObs)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- test[obs,]
probeExpanded <- matrix(probe, nrow = nrow(features), ncol = ncol(test), byrow = TRUE)
probeExpanded<-apply(probeExpanded,2,as.numeric)
# computing distances between the probe and exemplars in train dataset(features)
if (p %in% c(1,2)) {
distMatrix[,obs] <- ( rowSums((abs(features -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs,] <- apply(abs(features - probeExpanded), 1, max)
}
}
#Find k- nearest neighbors class
neighbors <- apply(as.matrix(distMatrix), 2, order)
neighbors <- neighbors[1:k,]
#rm(distMatrix)
n<-nrow(test)
neighbors.class<-sapply(1:n, function(i){
labels[neighbors[,i]]
})
# Frequency of each class
n.cat<-length(table(labels))
freq<-matrix(0, n, n.cat)
colnames(freq)<-names(table(labels))
for( i in 1:n){
categ<-table(match(as.factor(neighbors.class[,i]), colnames(freq)))
if(length(categ)>1) {
#freq[i,]<-categ
freq[i,as.numeric(names(categ))]<-categ
} else {
#freq[i,as.numeric(names(categ))]<-categ
freq[i,as.numeric(names(categ))]<-categ
}
}
# predicted label
PredLabels<- rep(NA,n)
for(i in 1:n){
PredLabels[i]<-colnames(freq)[which.max(freq[i,])]
}
#Probability of each class
prob<-freq/k
return(list(PredLabels=as.factor(PredLabels), prob=prob))
}
model<-kNN(data[,1:2], data[,1:2], data[,3], 5, 2)
prob<-model$prob[,2]
PredLabels<-model$PredLables
table(PredLabels)
model$PredLabels
PredLabels
model$PredLables
PredLabels<-model$PredLabels
prob<-model$prob[,2]
PredLabels<-model$PredLabels
#compute probability
#prob <- attr(model, "prob")
#prob <- ifelse(model=="1", prob, 1-prob)
prob15 <- matrix(prob, length(x), length(y))
#plot data
par(mar=rep(2,4))
contour(x, y, prob15, levels=0.5, labels="", xlab="", ylab="", main="knn boundaries")
points(data[,1:2], col=ifelse(data[,3]==1, "coral", "cornflowerblue"), pch=16)
points(new, pch=".", cex=1.2, col=ifelse(prob15>0.5, "coral", "cornflowerblue"))
legend("topright", inset=.05,  c("1","0"), col=c("coral", "cornflowerblue"), pch=16)
rm(list = ls())
#Version 4
kNN<-function(features, test,  labels, k, p){
# Compute distance
noObs <- nrow(test)
distMatrix <- matrix(NA, nrow(features), noObs)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- test[obs,]
probeExpanded <- matrix(probe, nrow = nrow(features), ncol = ncol(test), byrow = TRUE)
probeExpanded<-apply(probeExpanded,2,as.numeric)
# computing distances between the probe and exemplars in train dataset(features)
if (p %in% c(1,2)) {
distMatrix[,obs] <- ( rowSums((abs(features -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs,] <- apply(abs(features - probeExpanded), 1, max)
}
}
#Find k- nearest neighbors class
neighbors <- apply(as.matrix(distMatrix), 2, order)
neighbors <- neighbors[1:k,]
#rm(distMatrix)
n<-nrow(test)
neighbors.class<-sapply(1:n, function(i){
labels[neighbors[,i]]
})
# Frequency of each class
n.cat<-length(table(labels))
freq<-matrix(0, n, n.cat)
colnames(freq)<-names(table(labels))
for( i in 1:n){
categ<-table(match(as.factor(neighbors.class[,i]), colnames(freq)))
if(length(categ)>1) {
#freq[i,]<-categ
freq[i,as.numeric(names(categ))]<-categ
} else {
#freq[i,as.numeric(names(categ))]<-categ
freq[i,as.numeric(names(categ))]<-categ
}
}
# predicted label
PredLabels<- rep(NA,n)
for(i in 1:n){
PredLabels[i]<-colnames(freq)[which.max(freq[i,])]
}
#Probability of each class
prob<-freq/k
return(list(PredLabels=as.factor(PredLabels), prob=prob))
}
# CREATE DATA
genSun <- function(n = 200,
features = 2,
seed = NA, mus = NULL, sigma = NULL,
saveData = TRUE,
savePlot = TRUE) {
# Libraries
if (! require(mvtnorm)) { stop('required package not installed: mvtnorm') }
if (! require(ggplot2)) { stop('required package not installed: ggplot2') }
# For simplicity we restrict to 2 or 3 dimensions (this can be relaxed)
if (! as.numeric(features) %in% 2:3) {
stop('argument "features" must be 2 or 3.')
}
# Default values
if (! is.na(seed)) { set.seed(seed) }
if (is.null(sigma)) { sigma <- diag(features) }
if (is.null(mus)) { mus <- rep(0, features) }
# Simulate points from a bivariate normal
phi <- rmvnorm(n, mean = mus, sigma = sigma)
# Decide which belong to each cluster
rad <- (2 ** (features - 1) * gamma(1 + features / 2) /
(pi ** (features / 2))) ** (1 / features)
ones <- apply(phi, 1, function(x) { jitter(sum((x - mus) ** 2)) }) > rad ** 2
#ones <- apply(phi, 1, function(x) { sum((x - mus) ** 2) }) > rad ** 2
category <- rep(0, length = n)
category[ones] <- 1
# Build the final data frame
new.phi <- cbind.data.frame(phi, as.character(category))
new.phi[, 3] <- as.factor(new.phi[, 3])
colnames(new.phi) <- c("x1", "x2", 'y')
return(new.phi)
}
#train data
data<-genSun()
x<-seq(-3,3,length.out = 50)
y<-seq(-3,3, length.out = 50)
new <- expand.grid(x = x, y = y)
colnames(new)[1:2]<-colnames(data)[1:2]
#knn model
#model<- knn(data[,1:2], new, data[,3], k=15, prob=TRUE)
model<-kNN(data[,1:2], new, data[,3], 5, 2)
prob<-model$prob[,2]
PredLabels<-model$PredLabels
#compute probability
#prob <- attr(model, "prob")
#prob <- ifelse(model=="1", prob, 1-prob)
prob15 <- matrix(prob, length(x), length(y))
#plot data
par(mar=rep(2,4))
contour(x, y, prob15, levels=0.5, labels="", xlab="", ylab="", main="knn boundaries")
rm(list=ls())
kNN<-function(features, test,  labels, k, p){
# Compute distance
noObs <- nrow(test)
distMatrix <- matrix(NA, nrow(features), noObs)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- test[obs,]
probeExpanded <- matrix(probe, nrow = nrow(features), ncol = ncol(test), byrow = TRUE)
probeExpanded<-apply(probeExpanded,2,as.numeric)
# computing distances between the probe and exemplars in train dataset(features)
if (p %in% c(1,2)) {
distMatrix[,obs] <- ( rowSums((abs(features -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs,] <- apply(abs(features - probeExpanded), 1, max)
}
}
#Find k- nearest neighbors class
neighbors <- apply(as.matrix(distMatrix), 2, order)
neighbors <- neighbors[1:k,]
#rm(distMatrix)
n<-nrow(test)
neighbors.class<-sapply(1:n, function(i){
labels[neighbors[,i]]
})
# Frequency of each class
n.cat<-length(table(labels))
freq<-matrix(0, n, n.cat)
colnames(freq)<-names(table(labels))
for( i in 1:n){
categ<-table(match(as.factor(neighbors.class[,i]), colnames(freq)))
if(length(categ)>1) {
#freq[i,]<-categ
freq[i,as.numeric(names(categ))]<-categ
} else {
#freq[i,as.numeric(names(categ))]<-categ
freq[i,as.numeric(names(categ))]<-categ
}
}
# predicted label
PredLabels<- rep(NA,n)
for(i in 1:n){
PredLabels[i]<-colnames(freq)[which.max(freq[i,])]
}
#Probability of each class
prob<-freq/k
return(list(PredLabels=as.factor(PredLabels), prob=prob))
}
# CREATE DATA
genSun <- function(n = 200,
features = 2,
seed = NA, mus = NULL, sigma = NULL,
saveData = TRUE,
savePlot = TRUE) {
# Libraries
if (! require(mvtnorm)) { stop('required package not installed: mvtnorm') }
if (! require(ggplot2)) { stop('required package not installed: ggplot2') }
# For simplicity we restrict to 2 or 3 dimensions (this can be relaxed)
if (! as.numeric(features) %in% 2:3) {
stop('argument "features" must be 2 or 3.')
}
# Default values
if (! is.na(seed)) { set.seed(seed) }
if (is.null(sigma)) { sigma <- diag(features) }
if (is.null(mus)) { mus <- rep(0, features) }
# Simulate points from a bivariate normal
phi <- rmvnorm(n, mean = mus, sigma = sigma)
# Decide which belong to each cluster
rad <- (2 ** (features - 1) * gamma(1 + features / 2) /
(pi ** (features / 2))) ** (1 / features)
ones <- apply(phi, 1, function(x) { jitter(sum((x - mus) ** 2)) }) > rad ** 2
#ones <- apply(phi, 1, function(x) { sum((x - mus) ** 2) }) > rad ** 2
category <- rep(0, length = n)
category[ones] <- 1
# Build the final data frame
new.phi <- cbind.data.frame(phi, as.character(category))
new.phi[, 3] <- as.factor(new.phi[, 3])
colnames(new.phi) <- c("x1", "x2", 'y')
return(new.phi)
}
#train data
data<-genSun()
#test data
x<-seq(-3,3,length.out = 50)
y<-seq(-3,3, length.out = 50)
new <- expand.grid(x = x, y = y)
colnames(new)[1:2]<-colnames(data)[1:2]
#knn model
#model<- knn(data[,1:2], new, data[,3], k=15, prob=TRUE)
model<-kNN(data[,1:2], new, data[,3], 5, 2)
prob<-model$prob[,2]
PredLabels<-model$PredLabels
#compute probability
prob15 <- matrix(prob, length(x), length(y))
#plot data
par(mar=rep(2,4))
contour(x, y, prob15, levels=0.5, labels="", xlab="", ylab="", main="knn boundaries")
points(data[,1:2], col=ifelse(data[,3]==1, "coral", "cornflowerblue"), pch=16)
points(new, pch=".", cex=1.2, col=ifelse(prob15>0.5, "coral", "cornflowerblue"))
legend("topright", inset=.05,  c("1","0"), col=c("coral", "cornflowerblue"), pch=16)
model<-kNN(data[,1:2], new, data[,3], 7, 2)
prob<-model$prob[,2]
PredLabels<-model$PredLabels
#compute probability
prob15 <- matrix(prob, length(x), length(y))
#plot data
par(mar=rep(2,4))
contour(x, y, prob15, levels=0.5, labels="", xlab="", ylab="", main="knn boundaries")
points(data[,1:2], col=ifelse(data[,3]==1, "coral", "cornflowerblue"), pch=16)
points(new, pch=".", cex=1.2, col=ifelse(prob15>0.5, "coral", "cornflowerblue"))
legend("topright", inset=.05,  c("1","0"), col=c("coral", "cornflowerblue"), pch=16)
model<-kNN(data[,1:2], new, data[,3], 5, 2)
prob<-model$prob[,2]
PredLabels<-model$PredLabels
#compute probability
prob15 <- matrix(prob, length(x), length(y))
#plot data
par(mar=rep(2,4))
contour(x, y, prob15, levels=0.5, labels="", xlab="", ylab="", main="knn boundaries")
points(data[,1:2], col=ifelse(data[,3]==1, "coral", "cornflowerblue"), pch=16)
points(new, pch=".", cex=1.2, col=ifelse(prob15>0.5, "coral", "cornflowerblue"))
legend("topright", inset=.05,  c("1","0"), col=c("coral", "cornflowerblue"), pch=16)
newdata<-cbind(data, model$PredLabels, model$prob[,2])
model<-kNN(data[,1:2], data[,1:2], data[,3], 5, 2)
newdata<-cbind(data, model$PredLabels, model$prob[,2])
head(newdata)
prob<-model$prob[,2]
newdata<-cbind(data,PredLabels,prob)
model<-kNN(data[,1:2], data[,1:2], data[,3], 5, 2)
PredLabels<-model$PredLabels
prob<-model$prob[,2]
newdata<-cbind(data,PredLabels,prob)
head(newdata)
write.csv(newdata, 'predictions.csv', row.names = FALSE)
getwd()
pdf('plot.pdf')
par(mar=rep(2,4))
contour(x, y, prob15, levels=0.5, labels="", xlab="", ylab="", main="knn boundaries")
points(data[,1:2], col=ifelse(data[,3]==1, "coral", "cornflowerblue"), pch=16)
points(new, pch=".", cex=1.2, col=ifelse(prob15>0.5, "coral", "cornflowerblue"))
legend("topright", inset=.05,  c("1","0"), col=c("coral", "cornflowerblue"), pch=16)
dev.off()
rm(list=ls())
rm(list=ls())
#Get data
train <- read.csv("~/BGSE/15D012 Advanced Computational Methods/datasets/MNIST/MNIST_training.csv", header=FALSE)
test <- read.csv("~/BGSE/15D012 Advanced Computational Methods/datasets/MNIST/MNIST_test.csv", header=FALSE)
#valuation dataset
set.seed(666)
out<-sample(1:6000, 5000)
kNN
kNN<-function(features, test,  labels, k, p){
# Compute distance
noObs <- nrow(test)
distMatrix <- matrix(NA, nrow(features), noObs)
for (obs in 1:noObs) {
# getting the probe for the current observation
probe <- test[obs,]
probeExpanded <- matrix(probe, nrow = nrow(features), ncol = ncol(test), byrow = TRUE)
probeExpanded<-apply(probeExpanded,2,as.numeric)
# computing distances between the probe and exemplars in train dataset(features)
if (p %in% c(1,2)) {
distMatrix[,obs] <- ( rowSums((abs(features -
probeExpanded))^p) )^(1/p)
} else if (p==Inf) {
distMatrix[obs,] <- apply(abs(features - probeExpanded), 1, max)
}
}
#Find k- nearest neighbors class
neighbors <- apply(as.matrix(distMatrix), 2, order)
neighbors <- neighbors[1:k,]
#rm(distMatrix)
n<-nrow(test)
neighbors.class<-sapply(1:n, function(i){
labels[neighbors[,i]]
})
# Frequency of each class
n.cat<-length(table(labels))
freq<-matrix(0, n, n.cat)
colnames(freq)<-names(table(labels))
for( i in 1:n){
categ<-table(match(as.factor(neighbors.class[,i]), colnames(freq)))
if(length(categ)>1) {
#freq[i,]<-categ
freq[i,as.numeric(names(categ))]<-categ
} else {
#freq[i,as.numeric(names(categ))]<-categ
freq[i,as.numeric(names(categ))]<-categ
}
}
# predicted label
PredLabels<- rep(NA,n)
for(i in 1:n){
PredLabels[i]<-colnames(freq)[which.max(freq[i,])]
}
#Probability of each class
prob<-freq/k
return(list(PredLabels=as.factor(PredLabels), prob=prob))
}
kNN
mean
k<-c(1,3,5,7,9)
acc<-matrix(NA,1:3, length(k)); colnames(acc)<-k
acc
acc<-matrix(NA,3, length(k)); colnames(acc)<-k
acc
k<-c(1,3,5,7,9)
acc<-matrix(NA,3, length(k)); colnames(acc)<-k
for(i in 1:3){
for( j in k){}
#     model<-knn(train[-out,-1], train[out,-1],
#                train[-out,1], k=3)
model<-kNN(train[-out,-1], train[out,-1],
train[-out,1], k=j, p=i)
acc[i, k]<-mean(model==train[out,1])
print(c(i,k))
}
kNN
#valuation dataset
set.seed(666)
out<-sample(1:6000, 5000)
#Find best model
k<-c(1,3,5,7,9)
acc<-matrix(NA,3, length(k)); colnames(acc)<-k
for(i in 1:3){
for( j in k){
#     model<-knn(train[-out,-1], train[out,-1],
#                train[-out,1], k=3)
model<-kNN(train[-out,-1], train[out,-1],
train[-out,1], k=j, p=i)
acc[i, k]<-mean(model$PredLabels==train[out,1])
print(c(i,k))
}
}
model<-kNN(train[-out,-1], train[out,-1],
train[-out,1], k=3, p=2)
acc<-rep(NA,length(k))
acc
k<-c(1,3,5,7,9,11,13)
acc<-rep(NA,length(k))
acc
acc<-rep(NA,length(k)) ; names(acc)<-k
acc
for(i in k){
model<-knn(train[-out,-1], train[out,-1],
train[-out,1], k=i)
#     model<-kNN(train[-out,-1], train[out,-1],
#                 train[-out,1], k=j, p=i)
acc[i]<-mean(model==train[out,1])
print(i)
}
library(class)
for(i in k){
model<-knn(train[-out,-1], train[out,-1],
train[-out,1], k=i)
#     model<-kNN(train[-out,-1], train[out,-1],
#                 train[-out,1], k=j, p=i)
acc[i]<-mean(model==train[out,1])
print(i)
}
acc
acc<-rep(NA,length(k)) ; names(acc)<-k
for(i in 1:length(k)){
model<-knn(train[-out,-1], train[out,-1],
train[-out,1], k=k[i])
#     model<-kNN(train[-out,-1], train[out,-1],
#                 train[-out,1], k=j, p=i)
acc[i]<-mean(model==train[out,1])
print(i)
}
acc
plot(1:10, acc)
plot(acc)
predict(model, test)
model_new<-knn(train[-out,-1], test ,train[-out,1], k=k[i])
model_new
model_new<-knn(train[-out,-1], test ,train[-out,1], k=1)
attributes(model_new)
model_new$levels
predictions<-model_new
predictions
attributes(predictions)
attr(model_new, levels)
attr(model_new, 'levels')
predictions<-attr(model_new, 'levels')
predictions
attributes8predictions
attributes(predictions)
predictions
dim(tst)
dim(test)
model_new
predictions
attr(model_new, 'levels')
attributes(model_new)
attr(model_new, 'factor')
model_new
write.csv(model_new, 'MNIST_predictions.csv', row.names = FALSE)
write.csv(as.numeric(model_new), 'MNIST_predictions.csv', row.names = FALSE)
